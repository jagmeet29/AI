{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 322ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "new_model = tf.keras.models.load_model('best.keras')\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0) # 0 is the index of the built-in camera, change if you have multiple cameras\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read() # read a frame from the camera\n",
    "    cv2.imwrite('captured_image.jpg', frame)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "    'captured_image.jpg', target_size=(160, 160)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = new_model.predict(img_array)\n",
    "    #print(predictions)\n",
    "    score = tf.nn.relu(predictions)\n",
    "    predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "    class_names=['No', 'yes']\n",
    "\n",
    "    message = \"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    #print(predictions)\n",
    "    #print(message)\n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(message, frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('best.keras')\n",
    "new_model.save('best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jagme\\AppData\\Local\\Temp\\tmp0aurmyui\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jagme\\AppData\\Local\\Temp\\tmp0aurmyui\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model('best.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a .tflite file\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes with 232.75 \n",
      "No with 218.36 \n",
      "No with 220.59 \n",
      "No with 203.46 \n",
      "Yes with 188.86 \n",
      "No with 415.58 \n",
      "Yes with 708.03 \n",
      "Yes with 631.68 \n",
      "Yes with 594.00 \n",
      "Yes with 246.66 \n",
      "No with 249.97 \n",
      "No with 279.73 \n",
      "No with 279.02 \n",
      "Yes with 589.71 \n",
      "Yes with 623.89 \n",
      "Yes with 601.39 \n",
      "Yes with 307.05 \n",
      "Yes with 297.56 \n",
      "Yes with 229.99 \n",
      "Yes with 222.37 \n",
      "Yes with 294.33 \n",
      "Yes with 256.46 \n",
      "Yes with 211.99 \n",
      "Yes with 231.45 \n",
      "Yes with 292.17 \n",
      "Yes with 620.23 \n",
      "Yes with 559.17 \n",
      "Yes with 583.28 \n",
      "Yes with 617.84 \n",
      "No with 358.90 \n",
      "No with 349.21 \n",
      "No with 356.80 \n",
      "No with 381.85 \n",
      "No with 344.69 \n",
      "Yes with 454.78 \n",
      "Yes with 759.79 \n",
      "Yes with 726.39 \n",
      "No with 565.50 \n",
      "No with 252.81 \n",
      "No with 277.64 \n",
      "No with 557.18 \n",
      "Yes with 578.77 \n",
      "Yes with 688.90 \n",
      "Yes with 660.42 \n",
      "No with 404.23 \n",
      "No with 413.65 \n",
      "No with 402.91 \n",
      "No with 374.30 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;66;03m# read a frame from the camera\u001b[39;00m\n\u001b[0;32m     25\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptured_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_img(\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptured_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m)\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0) # 0 is the index of the built-in camera, change if you have multiple cameras\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read() # read a frame from the camera\n",
    "    cv2.imwrite('captured_image.jpg', frame)\n",
    "\n",
    "    time.sleep(.5)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "    'captured_image.jpg', target_size=(160, 160)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    # Set the tensor to point to the input data to be inferred\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the results\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    score = tf.nn.relu(predictions)\n",
    "    predictions = np.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "    class_names=['No', 'Yes']\n",
    "\n",
    "    message = \"{} with {:.2f} \".format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    cv2.putText(frame, message, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(message, frame)\n",
    "    print(message)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
